{"name":"A Huffman Codes Primer","tagline":"How to Create Your Own Huffman Encoding From Any Source Text","body":"### Why Do I Care About Encoding Text?\r\n\r\nIf you have ever heard of ASCII before, then you've heard of at least one text encoding method, a fairly simple one with 7 bits for each character. Java uses another one, that's 16-bits, but can be used to express a wider range of character types. These and other types of text encodings are used to transmit text as data over any kind of connection.  They are usually a fixed number of bits, with different unique configurations representing a different unique character.  \r\n\r\nNow if you're just transmitting a few words here and there, it's probably irrelevant what kind of encoding you're using. But what about if we'd like to transmit large amounts of text as efficiently as possible. That's where the number of bits you're using comes into play. If I have a very simple language that only contains 10 letters, I can represent those letters easily with just 4 bits. However, if I'm using ascii encoding to send them, I will end up with a lot of wasteful overhead. Similarly, even if I have a more complex language, it's likely that some of my characters will show up far more often than others, e.g. an 'e' in English text. If this is the case, it seems inefficient to be sending all my 'e's with just as many bits as my far less frequent letters. This is where the idea of Huffman codes come from.\r\n\r\n### What Does Huffman Code Mean?\r\n\r\nExtending this idea just a bit farther, it seems likely that we could come up with a variable encoding method, rather than fixed, for whatever language type we are planning to transmit. This would save us a lot of space if, for instance, we could figure out which characters are common, and represent those with very short bit combinations. In kind, we would need to have longer combinations for the less common characters, otherwise we would run out of possible combinations for all the characters in our language. But this is a good tradeoff for efficiency! (assuming we can accurately discover which are the most frequently used characters). And this is exactly what a Huffman Code refers to- it's a way of encoding a certain set of characters based on the frequencies by which they occur in the set, with the goal of minimizing bit length while keeping a unique combination available for every character. This may sound complicated at first. But with relatively simple data structures, it becomes achievable, and we're going to walk through it here step by step.\r\n\r\n### Constructing A Proper Encoding Scheme\r\n\r\nWith ASCII, or other common fixed bit encodings, we know exactly where a certain character starts and stops in an encoding, because they are all the same number of bits. So, if I see: _1101100110110011011110100000_, I can parse out each letter easily by just counting by 7 bits. With a Huffman code on the other hand, this isn't so easy, since the point is that the bit encodings are all different lengths. It must be a requirement then that no encoded character can be a prefix for the encoding of another character. That way once we see an encoding we recognize, we know to stop and convert it right there, rather than wondering if it's just the start of a longer character's encoding. With this uniqueness in mind, what's the best way to ","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}